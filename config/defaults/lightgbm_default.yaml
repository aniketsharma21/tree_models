# LightGBM Default Configuration
# Optimized for binary classification with fraud detection focus

# Model Configuration
model:
  model_type: lightgbm
  n_estimators: 300
  max_depth: 6
  learning_rate: 0.1
  random_state: 42
  early_stopping_rounds: 50
  verbose: false

  # LightGBM-specific parameters
  feature_fraction: 0.8
  bagging_fraction: 0.8
  bagging_freq: 5
  min_child_samples: 20
  reg_alpha: 0.1
  reg_lambda: 0.1
  objective: binary
  metric: auc
  boosting_type: gbdt

# Data Configuration
data:
  # Set these paths for your specific dataset
  train_path: null
  test_path: null
  output_dir: output/lightgbm_experiment

  # Column names
  target_col: target
  weight_col: sample_weight  # Set to null if no sample weights
  id_col: null

  # Data splitting
  test_size: 0.2
  valid_size: 0.2
  stratify: true

  # Preprocessing
  missing_strategy: median
  categorical_strategy: most_frequent
  encoding_strategy: label  # LightGBM handles categorical features natively
  scaling_strategy: null  # LightGBM doesn't need feature scaling
  use_knn_imputation: false

# Feature Selection Configuration
feature_selection:
  enable_feature_selection: true
  save_results: true

  # Variance filtering
  variance_threshold: 0.01

  # RFECV (Recursive Feature Elimination with Cross-Validation)
  rfecv_enabled: true
  rfecv_cv_folds: 5
  rfecv_scoring: roc_auc
  rfecv_step: 1
  rfecv_min_features: 5

  # Boruta (All-relevant feature selection)
  boruta_enabled: true
  boruta_max_iter: 100
  boruta_alpha: 0.05
  boruta_two_step: true

  # Consensus features
  min_agreement: 2

# Hyperparameter Tuning Configuration
tuning:
  enable_tuning: true
  n_trials: 100
  timeout: null

  # Cross-validation for tuning
  cv_folds: 5
  scoring: roc_auc

  # Optuna configuration
  sampler: tpe
  pruner: median
  n_startup_trials: 10
  n_warmup_steps: 10

  # Parallelization (LightGBM supports parallel training)
  n_jobs: -1

  # MLflow integration
  log_to_mlflow: true

  # Custom search space for LightGBM
  custom_search_space:
    # LightGBM-specific parameter ranges
    # num_leaves: [10, 300]
    # min_child_samples: [5, 100]
    # subsample_freq: [0, 10]

# Evaluation Configuration
evaluation:
  generate_plots: true
  save_results: true
  plot_format: png
  plot_dpi: 300

  # Primary metric
  primary_metric: auc_roc
  threshold: 0.5

  # Threshold optimization
  optimize_threshold: true
  threshold_metrics: [f1, precision, recall, youden]

  # Gains analysis
  gains_bins: 10

  # Cross-validation for evaluation
  eval_cv_folds: 5

# MLflow Configuration
mlflow:
  tracking_uri: null
  experiment_name: lightgbm_fraud_detection
  run_name: null

  # Logging preferences
  log_params: true
  log_metrics: true
  log_artifacts: true
  log_model: true
  log_plots: true

  # System information logging
  auto_log_system_info: true
  log_git_commit: true
  log_environment: true

# Global Settings
seed: 42
verbose: true
debug: false
parallel_jobs: -1  # Use all available cores for LightGBM
memory_limit: null

# LightGBM-specific Notes:
# - LightGBM is generally faster than XGBoost
# - Better handling of categorical features (no need for encoding)
# - Lower memory usage
# - Good performance with default parameters
# - Supports parallel training out of the box
